{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import fv3fit\n",
    "from vcm import DerivedMapping\n",
    "from vcm.catalog import catalog\n",
    "import numpy as np\n",
    "\n",
    "from loaders.mappers import open_nudge_to_fine\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from vcm import parse_datetime_from_str\n",
    "import vcm\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {\n",
    "    \"Base\": \"gs://vcm-ml-experiments/n2f-pire-stable-ml/2022-08-24/ablation-b22-data-sampling-higher-lr-no-lat-seed-0/\",\n",
    "    \"Base + latitude\": \"gs://vcm-ml-experiments/n2f-pire-stable-ml/2022-08-18/ablation-b22-data-sampling-higher-lr-seed-0/\",\n",
    "    \"Base + latitude + improved sampling\": \"gs://vcm-ml-experiments/n2f-pire-stable-ml/2022-08-16/ablation-higher-lr-seed-0/\",\n",
    "    \"Base + latitude + lower LR\": \"gs://vcm-ml-experiments/n2f-pire-stable-ml/2022-08-16/ablation-b22-data-sampling-seed-0\",\n",
    "    \"Base + latitude + improved sampling + lower LR\": \"gs://vcm-ml-experiments/n2f-pire-stable-ml/2022-05-17/tapering-effect-mae-no-taper-ensemble/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = {}\n",
    "for step, rundir in runs.items():\n",
    "    with fsspec.open(os.path.join(rundir, \"offline_diags\", \"tq_tendencies\", \"scalar_metrics.json\"), \"r\") as f:\n",
    "        metrics[step] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(metrics.keys())\n",
    "dQ1_col_int_R2 = []\n",
    "dQ2_col_int_R2 = []\n",
    "polar_dQ1_bias = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: {'mean': 0.17367746414761803, 'std': 0.006323067113701896}\n",
      "Base + latitude: {'mean': 0.18145564902158468, 'std': 0.0074711910188758465}\n",
      "Base + latitude + improved sampling: {'mean': 0.1773509485181309, 'std': 0.006837434967932587}\n",
      "Base + latitude + lower LR: {'mean': 0.27824892628496994, 'std': 0.009523459102442856}\n",
      "Base + latitude + improved sampling + lower LR: {'mean': 0.29144340847073585, 'std': 0.008304898439304131}\n",
      "\n",
      "Base: {'mean': 0.1478540474154636, 'std': 0.006050290804313889}\n",
      "Base + latitude: {'mean': 0.15604672217871, 'std': 0.005923853798441016}\n",
      "Base + latitude + improved sampling: {'mean': 0.14702832450291373, 'std': 0.006719409487227927}\n",
      "Base + latitude + lower LR: {'mean': 0.22176653248656003, 'std': 0.006255953148123605}\n",
      "Base + latitude + improved sampling + lower LR: {'mean': 0.2263754181937519, 'std': 0.0071515851451324}\n"
     ]
    }
   ],
   "source": [
    "for step, step_metrics in metrics.items():\n",
    "    print(f\"{step}: {step_metrics['column_integrated_dq1_r2_2d_global']}\")\n",
    "    dQ1_col_int_R2 .append(step_metrics['column_integrated_dq1_r2_2d_global']['mean'])\n",
    "\n",
    "print()\n",
    "\n",
    "for step, step_metrics in metrics.items():\n",
    "    print(f\"{step}: {step_metrics['column_integrated_dq2_r2_2d_global']}\")\n",
    "    dQ2_col_int_R2.append(step_metrics['column_integrated_dq2_r2_2d_global']['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = {}\n",
    "for step, rundir in runs.items():\n",
    "    models[step] = fv3fit.load(os.path.join(rundir, \"trained_models\", \"tq_tendencies\",) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = catalog[\"grid/c48\"].read()\n",
    "train_mapper = open_nudge_to_fine(\n",
    "    data_path = \"gs://vcm-ml-experiments/n2f-pire-sfc-updates/2022-01-21/nudged-run/fv3gfs_run/\",\n",
    "    nudging_variables = [\"air_temperature\", \"specific_humidity\", \"pressure_thickness_of_atmospheric_layer\"],\n",
    "    cache_size_mb=4000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [\n",
    "    (\"20200119.000000\",\"20200124.000000\"),\n",
    "    (\"20200114.000000\",\"20200129.000000\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = models[\"Base\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"tuple\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f6ead099ca18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtime_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparse_datetime_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mds_input_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDerivedMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m\"pressure_thickness_of_atmospheric_layer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mds_input_S\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDerivedMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"pressure_thickness_of_atmospheric_layer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"tuple\") to list"
     ]
    }
   ],
   "source": [
    "for i, (tmin, tmax) in enumerate(ranges):\n",
    "    keys = [key for key in train_mapper.keys() if ((key>tmin) and (key<tmax))]\n",
    "    time_coords = [parse_datetime_from_str(key) for key in keys]\n",
    "    ds = xr.concat([train_mapper[key] for key in keys], pd.Index(time_coords, name=\"time\"))\n",
    "    ds_input_N = DerivedMapping(ds.merge(grid)).dataset(full_model.input_variables + full_model.output_variables +  [ \"pressure_thickness_of_atmospheric_layer\",]) \\\n",
    "        .where(grid.lat > 60)\n",
    "    ds_input_S = DerivedMapping(ds.merge(grid)).dataset(full_model.input_variables + full_model.output_variables +  [ \"pressure_thickness_of_atmospheric_layer\",]) \\\n",
    "        .where(grid.lat < -60)\n",
    "    ds_input_N_stacked = ds_input_N.stack(sample=[\"x\", \"y\", \"tile\", \"time\"]) \\\n",
    "        .dropna('sample') \\\n",
    "        .transpose(\"sample\", ...)\n",
    "    ds_input_S_stacked = ds_input_S.stack(sample=[\"x\", \"y\", \"tile\", \"time\"]) \\\n",
    "        .dropna('sample') \\\n",
    "        .transpose(\"sample\", ...)\n",
    "    ds_input_N_stacked.reset_index('sample', drop=True).to_netcdf(f\"temp/N_{i}.nc\")\n",
    "    ds_input_S_stacked.reset_index('sample', drop=True).to_netcdf(f\"temp/S_{i}.nc\")\n",
    "    \n",
    "    del ds_input_N_stacked, ds_input_S_stacked, ds_input_N, ds_input_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [os.path.join(\"temp\",file) for file in os.listdir(\"temp\") if file.endswith(\".nc\") and file.startswith(\"N\")]\n",
    "\n",
    "sample_count = 0\n",
    "pole_test_data = []\n",
    "for file in data_files:\n",
    "    ds_ = xr.open_dataset(file)\n",
    "    ds_.assign_coords({\"sample\": range(sample_count, sample_count+len(ds_.sample))})\n",
    "    sample_count += len(ds_.sample)\n",
    "    pole_test_data .append(ds_)\n",
    "\n",
    "pole_test_data = xr.concat(pole_test_data, dim=\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base 1.1158522423034133e-06\n",
      "Base + latitude 8.105506845318954e-07\n",
      "Base + latitude + improved sampling 6.992832160864569e-07\n",
      "Base + latitude + lower LR -1.7640969219544372e-07\n",
      "Base + latitude + improved sampling + lower LR 2.1738980763588844e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_files = [os.path.join(\"temp\",file) for file in os.listdir(\"temp\") if file.endswith(\".nc\") ]\n",
    "\n",
    "sample_count = 0\n",
    "pole_test_data = []\n",
    "for file in data_files:\n",
    "    ds_ = xr.open_dataset(file)\n",
    "    ds_.assign_coords({\"sample\": range(sample_count, sample_count+len(ds_.sample))})\n",
    "    sample_count += len(ds_.sample)\n",
    "    pole_test_data .append(ds_)\n",
    "\n",
    "pole_test_data = xr.concat(pole_test_data, dim=\"sample\")\n",
    "predictions = {\n",
    "    step: step_model.predict(pole_test_data) for step, step_model in models.items()\n",
    "}\n",
    "biases = {\n",
    "    step: step_predictions[[\"dQ1\", \"dQ2\"]]-pole_test_data[[\"dQ1\", \"dQ2\"]]\n",
    "    for step, step_predictions in predictions.items()\n",
    "}\n",
    "interp_dQ1_bias = {}\n",
    "for step, bias in biases.items():\n",
    "    bias_dQ1_plev = vcm.interpolate_to_pressure_levels(\n",
    "        bias['dQ1'],\n",
    "        pole_test_data[\"pressure_thickness_of_atmospheric_layer\"],\n",
    "        dim=\"z\",\n",
    "        #levels=np.array([20000.,]),\n",
    "    )\n",
    "    interp_dQ1_bias[step] = bias_dQ1_plev\n",
    "    #print(step, bias_dQ1_200hPa.sel(pressure=20000).values.mean())\n",
    "    \n",
    "    print(step, bias_dQ1_plev.sel(pressure=slice(15000, 40000)).mean(\"pressure\").values.mean())\n",
    "    polar_dQ1_bias.append( bias_dQ1_plev.sel(pressure=slice(15000, 40000)).mean(\"pressure\").values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    list(zip(names, dQ1_col_int_R2, dQ2_col_int_R2, polar_dQ1_bias)),\n",
    "    columns =[\n",
    "        'Name', \n",
    "        'Column-integrated dQ1 $R^2$', \n",
    "        'Column-integrated dQ2 $R^2$', \n",
    "        '150-400 hPa mean dQ1 bias ($\\lvert \\mathrm{lat} \\rvert > 60^{\\circ}$, days 0-10) [K/s]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_latex('sensitivity_table.tex', formatters={'cost':'${:,.2f}'.format})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Column-integrated dQ1 $R^2$</th>\n",
       "      <th>Column-integrated dQ2 $R^2$</th>\n",
       "      <th>150-400 hPa mean dQ1 bias ($\\lvert \\mathrm{lat} \\rvert &gt; 60^{\\circ}$, days 0-10) [K/s]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.173677</td>\n",
       "      <td>0.147854</td>\n",
       "      <td>1.115852e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Base + latitude</td>\n",
       "      <td>0.181456</td>\n",
       "      <td>0.156047</td>\n",
       "      <td>8.105507e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base + latitude + improved sampling</td>\n",
       "      <td>0.177351</td>\n",
       "      <td>0.147028</td>\n",
       "      <td>6.992832e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Base + latitude + lower LR</td>\n",
       "      <td>0.278249</td>\n",
       "      <td>0.221767</td>\n",
       "      <td>-1.764097e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Base + latitude + improved sampling + lower LR</td>\n",
       "      <td>0.291443</td>\n",
       "      <td>0.226375</td>\n",
       "      <td>2.173898e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  \\\n",
       "0                                            Base   \n",
       "1                                 Base + latitude   \n",
       "2             Base + latitude + improved sampling   \n",
       "3                      Base + latitude + lower LR   \n",
       "4  Base + latitude + improved sampling + lower LR   \n",
       "\n",
       "   Column-integrated dQ1 $R^2$  Column-integrated dQ2 $R^2$  \\\n",
       "0                     0.173677                     0.147854   \n",
       "1                     0.181456                     0.156047   \n",
       "2                     0.177351                     0.147028   \n",
       "3                     0.278249                     0.221767   \n",
       "4                     0.291443                     0.226375   \n",
       "\n",
       "   150-400 hPa mean dQ1 bias ($\\lvert \\mathrm{lat} \\rvert > 60^{\\circ}$, days 0-10) [K/s]  \n",
       "0                                       1.115852e-06                                       \n",
       "1                                       8.105507e-07                                       \n",
       "2                                       6.992832e-07                                       \n",
       "3                                      -1.764097e-07                                       \n",
       "4                                       2.173898e-07                                       "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv3net_kernel",
   "language": "python",
   "name": "fv3net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
